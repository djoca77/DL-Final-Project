{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqtgqmuWD0hZ"
   },
   "source": [
    "# HW3: CIFAR10: Convolutional Neural Networks\n",
    "In this homework assignment, we'll be focusing on all things CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uspvkrjEIaJ",
    "outputId": "9ae97a9e-721e-47e5-96f2-26c50a73256b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \r\n",
      "[Clang 13.0.1 ]\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VczJRb92GLdW"
   },
   "source": [
    "If you are running the notebook on Colab, you need to mount your drive. You can then add the directory where you python code is to the system path. The change in the system path is valid only for this session. If you are running the notebook in your local machine, the boolean variable `isColab` is going to be `False`, so anything inside the if statement will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJJyIWTV9Yo_"
   },
   "source": [
    "Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y7s9UO2FGug0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x8EOJ099bO5"
   },
   "source": [
    "Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DRRrF7J9kma"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import   assignment, conv_model, layers_keras, layers_manual\n",
    "%aimport assignment, conv_model, layers_keras, layers_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iji6pCKuut8R"
   },
   "source": [
    "Data Pathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eehxvvoS-bP4"
   },
   "source": [
    "## Data Preprocessing: CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnxamXQ25WR_"
   },
   "source": [
    "This code block will help you get familiar with the shape and type of the data returned by `get_data()`. `get_data` returns X0 (training images), X1 (training labels), Y0 (testing images), Y1 (testing labels), and some additional info about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "fl_FtgmC-fxl",
    "outputId": "aee17b99-97ea-483e-ddd2-3f57a53cfa1a"
   },
   "outputs": [],
   "source": [
    "data = assignment.get_data()\n",
    "X0, Y0, X1, Y1, D0, D1, D_info = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_info.features['label']._int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(D0, D_info);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've completed **[TODO 1]** in `get_default_CNN_model()`, you can run the cell below to visualize what your data augmentation pipeline is doing to the images. This will also hopefully help you determine which augmentations may help your model generalize and which will increase performance!\n",
    "- First row shows original images but scaled\n",
    "- Second row shows images after they have been preprocessed\n",
    "- Third row shows your augmented images. \n",
    "\n",
    "NOTE: You do not need to finish TODO 2 before running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## You can use any list of 10 indices\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sample_image_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mgather(X0, sample_image_indices), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m sample_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(Y0, sample_image_indices)\n\u001b[1;32m      8\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X0' is not defined"
     ]
    }
   ],
   "source": [
    "import conv_model\n",
    "\n",
    "## You can use any list of 10 indices\n",
    "sample_image_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "sample_images = tf.cast(tf.gather(X0, sample_image_indices), tf.float32)\n",
    "sample_labels = tf.gather(Y0, sample_image_indices)\n",
    "\n",
    "fig, ax = plt.subplots(3, 10)\n",
    "fig.set_size_inches(24, 8)\n",
    "\n",
    "args = conv_model.get_default_CNN_model()\n",
    "\n",
    "preprocessed_images = args.model.input_prep_fn(sample_images)\n",
    "augmented_images = args.model.augment_fn(preprocessed_images)\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0][i].imshow(sample_images[i]/255., cmap = \"Greys\")\n",
    "    ax[1][i].imshow(preprocessed_images[i], cmap = \"Greys\")\n",
    "    ax[2][i].imshow(augmented_images[i], cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train A Basic Keras Model\n",
    "\n",
    "As part of step 2 from the handout, we just want you to construct a simple keras model to run prediction on your dataset!\n",
    "\n",
    "Implement **[TODO 2]** in `get_default_CNN_model` to return a CNN model that can train above an accuracy of 55% (note that the requirement for 1470 is 62% and for 2470, is 65% though). Feel free to play around with the number of layers, hyperparameters for layers, epochs, batch size, and anything else you can think of. \n",
    "\n",
    "**Requirements:**\n",
    "- Model must contain Conv2D, BatchNormalization, and Dropout layers. \n",
    "- These must be imported from the argument namespaces (already done by default).\n",
    "- Task 1 will automatically use `tf.keras.layers` implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01massignment\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## You can test with more epochs later\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m assignment\u001b[38;5;241m.\u001b[39mrun_task(data, \u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import assignment \n",
    "\n",
    "## You can test with more epochs later\n",
    "cnn_model = assignment.run_task(data, 1, epochs=30, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Your Own Layers\n",
    "For steps 3, 4, and 5 from the handout, you'll need to implement the layers from scratch inside of `layers_keras.py`.\n",
    "Feel free to refer to the official documentation for how these methods are supposed to function. \n",
    "More details are included in the layer block comments, and the init methods are already provided. \n",
    "\n",
    "**Requirements**:\n",
    "- Implement Conv2D, BatchNormalization, and Dropout in `layers_keras.py`\n",
    "- Cannot use existing layers as sub-components. \n",
    "- Cannot use `tf.nn.batch_normalization` or `tf.nn.dropout`. \n",
    "- CAN use `tf.nn.convolution`...\n",
    "- Should utilize all non-commented-out arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Conv2D runs without erroring. This does not guarantee that your forward pass calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[1;32m      7\u001b[0m conv_layer \u001b[38;5;241m=\u001b[39m layers_keras\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[1;32m     11\u001b[0m conv_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dl3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.random.uniform((1, 4, 4, 3), 0, 10, dtype=tf.float32)\n",
    "\n",
    "seed = 8675309\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = layers_keras.Conv2D(1, 2, strides=2)\n",
    "print(\"Output:\", conv_layer(random_input, training=True))\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = tf.keras.layers.Conv2D(1, 2, strides=2)\n",
    "print('Expected:', conv_layer(random_input, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Batch Normalization runs without erroring. This does not guarantee that your forward pass calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.random.uniform((3,3), 0, 10, dtype=tf.float32)\n",
    "print(\"Input:\", random_input)\n",
    "\n",
    "batch_norm = layers_keras.BatchNormalization()\n",
    "print(\"Output:\", batch_norm(random_input, training=True))\n",
    "\n",
    "batch_norm = tf.keras.layers.BatchNormalization()\n",
    "print('Expected:', batch_norm(random_input, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Dropout runs without erroring. This does not guarantee that your forward pass or input gradients calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n",
      "\n",
      "Output Training:\n",
      "tf.Tensor(\n",
      "[[1.25 1.25 1.25 1.25 1.25 1.25 0.   1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 1.25 1.25 0.   1.25 0.   0.   1.25 1.25]], shape=(2, 11), dtype=float32)\n",
      "Expected Training:\n",
      "tf.Tensor(\n",
      "[[1.25 1.25 1.25 1.25 1.25 1.25 0.   1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 1.25 1.25 0.   1.25 0.   0.   1.25 1.25]], shape=(2, 11), dtype=float32)\n",
      "\n",
      "Output Testing:\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n",
      "Expected Testing:\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.ones((2, 11))\n",
    "print(\"Input:\\n\", random_input)\n",
    "\n",
    "seed = 8675309\n",
    "for mode_str, mode in zip(['Training', 'Testing'], [True, False]):\n",
    "    print()\n",
    "    for layer_str, layer in zip(['Output','Expected'], [layers_keras.Dropout, tf.keras.layers.Dropout]):\n",
    "        tf.random.set_seed(seed)\n",
    "        dropout_fn = layer(rate=0.2)\n",
    "        print(f'{layer_str} {mode_str}:')\n",
    "        print(dropout_fn(random_input, training=mode))\n",
    "\n",
    "# Expected: Around rate% of the entries should be zeros in training mode.\n",
    "#   Should also be normalized such that, on average, magnitude perserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model\n",
    "Now, let's see if your model works with the new components in place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assignment \n",
    "\n",
    "# assignment.run_task(data, 2, 1, epochs=2)   ## Just manual conv\n",
    "# assignment.run_task(data, 2, 2, epochs=2)   ## Just manual bnorm\n",
    "# assignment.run_task(data, 2, 3, epochs=2)   ## Just manual dropout\n",
    "assignment.run_task(data, 2, epochs=2)        ## Test all 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Convolution!\n",
    "\n",
    "Now, go ahead and implement convolution manually! This should be done inside of the `layers_manual.py` file. It's very non-trivial to perform convolution differentiably without using `tf.nn.convolution`, so the manual convolution should only run during inference time. Below is a quick test to see if your convolution is consistent with the Keras layered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers_manual\n",
    "\n",
    "random_input = tf.random.uniform((2, 4, 4, 3), 0, 10, dtype=tf.float32)\n",
    "\n",
    "seed = 8675309\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = layers_manual.Conv2D(1, (2, 2), strides=2, padding='valid')\n",
    "print(\"Output:\", conv_layer(random_input, training=False))\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = tf.keras.layers.Conv2D(1, (2, 2), strides=2, padding='valid')\n",
    "print('Expected:', conv_layer(random_input, training=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the loop, this will happen at the end of every epoch because a validation set is being evaluated alongside your training set. The following will test it out for you! Don't worry if your categorical accuracy looks low here. As long as everything works without erroring, feel free to move on and test the whole model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assignment \n",
    "\n",
    "assignment.run_task(data, 3, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Make sure your model runs and trains up to standards! When you find a model configuration that you like, feel free to update your `get_default_CNN_model` function so that the autograder can use it with your arguments. If your model takes too long to train (> 10 mins), the autograder may time out, so take consideration of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run at least once\n",
    "from types import SimpleNamespace\n",
    "from conv_model import CustomSequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you can copy your code here for quick testing!\n",
    "\n",
    "Make sure to put it back into your `conv_model.py` file for the autograder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_CNN_model(\n",
    "    conv_ns=tf.keras.layers,\n",
    "    norm_ns=tf.keras.layers,\n",
    "    drop_ns=tf.keras.layers,\n",
    "    man_conv_ns=tf.keras.layers,\n",
    "):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ns = tf.keras.layers\n",
    "norm_ns = tf.keras.layers\n",
    "drop_ns = tf.keras.layers\n",
    "man_conv_ns = tf.keras.layers\n",
    "\n",
    "args = get_default_CNN_model(\n",
    "    conv_ns=conv_ns, \n",
    "    norm_ns=norm_ns, \n",
    "    drop_ns=drop_ns, \n",
    "    man_conv_ns=man_conv_ns\n",
    ")\n",
    "\n",
    "history = args.model.fit(\n",
    "    X0,\n",
    "    Y0,\n",
    "    epochs=args.epochs,\n",
    "    batch_size=args.batch_size,\n",
    "    validation_data=(X1, Y1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "In case you need them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.argmax(cnn_model.predict(X1), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P1, Y1)\n",
    "\n",
    "P0 = np.argmax(cnn_model.predict(X0), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P0, Y0)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(confusion_mtx, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.argmax(cnn_model.predict(X1), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P1, Y1)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(confusion_mtx, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 10)\n",
    "fig.set_size_inches(24, 8)\n",
    "\n",
    "pred0 = cnn_model.predict(X0[:10])\n",
    "pred1 = cnn_model.predict(X1[:10])\n",
    "\n",
    "def p2l(pred):\n",
    "    return D_info.features['label']._int2str[pred]\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0][i].imshow(X0[i], cmap = \"Greys\")\n",
    "    ax[1][i].imshow(X1[i], cmap = \"Greys\")\n",
    "    ax[1][i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    ax[0][i].set_xlabel(f\"Pred {p2l(np.argmax(pred0[i], -1))} | {p2l(Y0[i])}\")    \n",
    "    ax[1][i].set_xlabel(f\"Pred {p2l(np.argmax(pred1[i], -1))} | {p2l(Y1[i])}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "23b34023921adc9bdc04a0c05af3f3fbcc2878566e4710a2dfda201d240972ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
